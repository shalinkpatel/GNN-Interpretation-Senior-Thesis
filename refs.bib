
@misc{ying_gnnexplainer_2019,
	title = {{GNNExplainer}: Generating Explanations for Graph Neural Networks},
	url = {http://arxiv.org/abs/1903.03894},
	doi = {10.48550/arXiv.1903.03894},
	shorttitle = {{GNNExplainer}},
	abstract = {Graph Neural Networks ({GNNs}) are a powerful tool for machine learning on graphs.{GNNs} combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by {GNNs} remains unsolved. Here we propose {GNNExplainer}, the first general, model-agnostic approach for providing interpretable explanations for predictions of any {GNN}-based model on any graph-based machine learning task. Given an instance, {GNNExplainer} identifies a compact subgraph structure and a small subset of node features that have a crucial role in {GNN}'s prediction. Further, {GNNExplainer} can generate consistent and concise explanations for an entire class of instances. We formulate {GNNExplainer} as an optimization task that maximizes the mutual information between a {GNN}'s prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1\% on average. {GNNExplainer} provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty {GNNs}.},
	number = {{arXiv}:1903.03894},
	publisher = {{arXiv}},
	author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
	urldate = {2023-03-14},
	date = {2019-11-13},
	eprinttype = {arxiv},
	eprint = {1903.03894 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/79G86HQS/Ying et al. - 2019 - GNNExplainer Generating Explanations for Graph Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/5WUE7UYQ/1903.html:text/html},
}

@misc{luo_parameterized_2020,
	title = {Parameterized Explainer for Graph Neural Network},
	url = {http://arxiv.org/abs/2011.04573},
	doi = {10.48550/arXiv.2011.04573},
	abstract = {Despite recent progress in Graph Neural Networks ({GNNs}), explaining predictions made by {GNNs} remains a challenging open problem. The leading method independently addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a {GNN} model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned {GNN} model, leading to a lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class). In this study, we address these key challenges and propose {PGExplainer}, a parameterized explainer for {GNNs}. {PGExplainer} adopts a deep neural network to parameterize the generation process of explanations, which enables {PGExplainer} a natural approach to explaining multiple instances collectively. Compared to the existing work, {PGExplainer} has better generalization ability and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7{\textbackslash}\% relative improvement in {AUC} on explaining graph classification over the leading baseline.},
	number = {{arXiv}:2011.04573},
	publisher = {{arXiv}},
	author = {Luo, Dongsheng and Cheng, Wei and Xu, Dongkuan and Yu, Wenchao and Zong, Bo and Chen, Haifeng and Zhang, Xiang},
	urldate = {2023-03-14},
	date = {2020-11-09},
	eprinttype = {arxiv},
	eprint = {2011.04573 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/3IZJBY7Y/Luo et al. - 2020 - Parameterized Explainer for Graph Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/UEIKD4SP/2011.html:text/html},
}

@article{moerman_grnboost2_2019,
	title = {{GRNBoost}2 and Arboreto: efficient and scalable inference of gene regulatory networks},
	volume = {35},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/bty916},
	doi = {10.1093/bioinformatics/bty916},
	shorttitle = {{GRNBoost}2 and Arboreto},
	abstract = {Inferring a Gene Regulatory Network ({GRN}) from gene expression data is a computationally expensive task, exacerbated by increasing data sizes due to advances in high-throughput gene profiling technology, such as single-cell {RNA}-seq. To equip researchers with a toolset to infer {GRNs} from large expression datasets, we propose {GRNBoost}2 and the Arboreto framework. {GRNBoost}2 is an efficient algorithm for regulatory network inference using gradient boosting, based on the {GENIE}3 architecture. Arboreto is a computational framework that scales up {GRN} inference algorithms complying with this architecture. Arboreto includes both {GRNBoost}2 and an improved implementation of {GENIE}3, as a user-friendly open source Python package.Arboreto is available under the 3-Clause {BSD} license at http://arboreto.readthedocs.io.Supplementary data are available at Bioinformatics online.},
	pages = {2159--2161},
	number = {12},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Moerman, Thomas and Aibar Santos, Sara and Bravo González-Blas, Carmen and Simm, Jaak and Moreau, Yves and Aerts, Jan and Aerts, Stein},
	urldate = {2023-03-14},
	date = {2019-06-15},
	file = {Full Text PDF:/Users/shalinpatel/Zotero/storage/ZD6M9CTH/Moerman et al. - 2019 - GRNBoost2 and Arboreto efficient and scalable inf.pdf:application/pdf;Snapshot:/Users/shalinpatel/Zotero/storage/XKU5SQLY/5184284.html:text/html},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	shorttitle = {{PyTorch}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. {PyTorch} is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as {GPUs}. In this paper, we detail the principles that drove the implementation of {PyTorch} and how they are reflected in its architecture. We emphasize that every aspect of {PyTorch} is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of {PyTorch} on several common benchmarks.},
	number = {{arXiv}:1912.01703},
	publisher = {{arXiv}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and {DeVito}, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	urldate = {2023-03-14},
	date = {2019-12-03},
	eprinttype = {arxiv},
	eprint = {1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/Y6APV9BY/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/UG3LGATS/1912.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2023-03-14},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/UC2VTKA2/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/VTVYHDKP/1412.html:text/html},
}

@misc{bingham_pyro_2018,
	title = {Pyro: Deep Universal Probabilistic Programming},
	url = {http://arxiv.org/abs/1810.09538},
	doi = {10.48550/arXiv.1810.09538},
	shorttitle = {Pyro},
	abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in {AI} research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of {PyTorch}, a modern {GPU}-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
	number = {{arXiv}:1810.09538},
	publisher = {{arXiv}},
	author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
	urldate = {2023-03-14},
	date = {2018-10-18},
	eprinttype = {arxiv},
	eprint = {1810.09538 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/LGCDUSM9/Bingham et al. - 2018 - Pyro Deep Universal Probabilistic Programming.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/UYPKSQM3/1810.html:text/html},
}

@article{kobyzev_normalizing_2021,
	title = {Normalizing Flows: An Introduction and Review of Current Methods},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1908.09257},
	doi = {10.1109/TPAMI.2020.2992934},
	shorttitle = {Normalizing Flows},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	pages = {3964--3979},
	number = {11},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	urldate = {2023-03-14},
	date = {2021-11-01},
	eprinttype = {arxiv},
	eprint = {1908.09257 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/X7SCTINI/Kobyzev et al. - 2021 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/ZLFLE2CJ/1908.html:text/html},
}

@article{petralia_new_2016,
	title = {New Method for Joint Network Analysis Reveals Common and Different Coexpression Patterns among Genes and Proteins in Breast Cancer},
	volume = {15},
	issn = {1535-3893},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4782177/},
	doi = {10.1021/acs.jproteome.5b00925},
	abstract = {, We focus on characterizing
common and different coexpression patterns
among {RNAs} and proteins in breast cancer tumors. To address this problem,
we introduce Joint Random Forest ({JRF}), a novel nonparametric algorithm
to simultaneously estimate multiple coexpression networks by effectively
borrowing information across protein and gene expression data. The
performance of {JRF} was evaluated through extensive simulation studies
using different network topologies and data distribution functions.
Advantages of {JRF} over other algorithms that estimate class-specific
networks separately were observed across all simulation settings.
{JRF} also outperformed a competing method based on Gaussian graphic
models. We then applied {JRF} to simultaneously construct gene and protein
coexpression networks based on protein and {RNAseq} data from {CPTAC}-{TCGA}
breast cancer study. We identified interesting common and differential
coexpression patterns among genes and proteins. This information can
help to cast light on the potential disease mechanisms of breast cancer.},
	pages = {743--754},
	number = {3},
	journaltitle = {Journal of Proteome Research},
	shortjournal = {J Proteome Res},
	author = {Petralia, Francesca and Song, Won-Min and Tu, Zhidong and Wang, Pei},
	urldate = {2023-03-14},
	date = {2016-03-04},
	pmid = {26733076},
	pmcid = {PMC4782177},
	file = {PubMed Central Full Text PDF:/Users/shalinpatel/Zotero/storage/WE2ZVVG9/Petralia et al. - 2016 - New Method for Joint Network Analysis Reveals Comm.pdf:application/pdf},
}

@inproceedings{cho_friendship_2011,
	location = {New York, {NY}, {USA}},
	title = {Friendship and mobility: user movement in location-based social networks},
	isbn = {978-1-4503-0813-7},
	url = {https://doi.org/10.1145/2020408.2020579},
	doi = {10.1145/2020408.2020579},
	series = {{KDD} '11},
	shorttitle = {Friendship and mobility},
	abstract = {Even though human movement and mobility patterns have a high degree of freedom and variation, they also exhibit structural patterns due to geographic and social constraints. Using cell phone location data, as well as data from two online location-based social networks, we aim to understand what basic laws govern human motion and dynamics. We find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks. Short-ranged travel is periodic both spatially and temporally and not effected by the social network structure, while long-distance travel is more influenced by social network ties. We show that social relationships can explain about 10\% to 30\% of all human movement, while periodic behavior explains 50\% to 70\%. Based on our findings, we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure. We show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility.},
	pages = {1082--1090},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Cho, Eunjoon and Myers, Seth A. and Leskovec, Jure},
	urldate = {2023-03-14},
	date = {2011-08-21},
	keywords = {communication networks, human mobility, social networks},
	file = {Full Text PDF:/Users/shalinpatel/Zotero/storage/EXA7D3DQ/Cho et al. - 2011 - Friendship and mobility user movement in location.pdf:application/pdf},
}

@article{washio_state_2003,
	title = {State of the art of graph-based data mining},
	volume = {5},
	issn = {1931-0145},
	url = {https://dl.acm.org/doi/10.1145/959242.959249},
	doi = {10.1145/959242.959249},
	abstract = {The need for mining structured data has increased in the past few years. One of the best studied data structures in computer science and discrete mathematics are graphs. It can therefore be no surprise that graph based data mining has become quite popular in the last few years.This article introduces the theoretical basis of graph based data mining and surveys the state of the art of graph-based data mining. Brief descriptions of some representative approaches are provided as well.},
	pages = {59--68},
	number = {1},
	journaltitle = {{ACM} {SIGKDD} Explorations Newsletter},
	shortjournal = {{SIGKDD} Explor. Newsl.},
	author = {Washio, Takashi and Motoda, Hiroshi},
	urldate = {2023-03-24},
	date = {2003-07-01},
	keywords = {data mining, graph, graph-based data mining, path, structured data, tree},
	file = {Full Text PDF:/Users/shalinpatel/Zotero/storage/RQ6KK2PI/Washio and Motoda - 2003 - State of the art of graph-based data mining.pdf:application/pdf},
}

@misc{defferrard_convolutional_2017,
	title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
	url = {http://arxiv.org/abs/1606.09375},
	doi = {10.48550/arXiv.1606.09375},
	abstract = {In this work, we are interested in generalizing convolutional neural networks ({CNNs}) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of {CNNs} in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical {CNNs}, while being universal to any graph structure. Experiments on {MNIST} and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	number = {{arXiv}:1606.09375},
	publisher = {{arXiv}},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	urldate = {2023-03-24},
	date = {2017-02-05},
	eprinttype = {arxiv},
	eprint = {1606.09375 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/S56T2CRR/Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/3T2ADBQ6/1606.html:text/html},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A Gentle Introduction to Graph Neural Networks},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	pages = {10.23915/distill.00033},
	number = {8},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alex},
	urldate = {2023-03-24},
	date = {2021-08-17},
}

@article{daigavane_understanding_2021,
	title = {Understanding Convolutions on Graphs},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/understanding-gnns},
	doi = {10.23915/distill.00032},
	pages = {10.23915/distill.00032},
	number = {8},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
	urldate = {2023-03-24},
	date = {2021-08-17},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {http://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	number = {{arXiv}:1609.02907},
	publisher = {{arXiv}},
	author = {Kipf, Thomas N. and Welling, Max},
	urldate = {2023-03-24},
	date = {2017-02-22},
	eprinttype = {arxiv},
	eprint = {1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/MJD9C892/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/3N3GDCYC/1609.html:text/html},
}

@article{jospin_hands-bayesian_2022,
	title = {Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users},
	volume = {17},
	issn = {1556-603X, 1556-6048},
	url = {http://arxiv.org/abs/2007.06823},
	doi = {10.1109/MCI.2022.3155327},
	abstract = {Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian Neural Networks, i.e. Stochastic Artificial Neural Networks trained using Bayesian methods.},
	pages = {29--48},
	number = {2},
	journaltitle = {{IEEE} Computational Intelligence Magazine},
	shortjournal = {{IEEE} Comput. Intell. Mag.},
	author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
	urldate = {2023-03-24},
	date = {2022-05},
	eprinttype = {arxiv},
	eprint = {2007.06823 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 62-02 (Primary), G.3, I.2.6},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/YHYBBJNX/Jospin et al. - 2022 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/959XTLWV/2007.html:text/html},
}

@misc{betancourt_conceptual_2018,
	title = {A Conceptual Introduction to Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1701.02434},
	doi = {10.48550/arXiv.1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	number = {{arXiv}:1701.02434},
	publisher = {{arXiv}},
	author = {Betancourt, Michael},
	urldate = {2023-04-02},
	date = {2018-07-15},
	eprinttype = {arxiv},
	eprint = {1701.02434 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/F6XVC5SS/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/ZYF7KZ9F/1701.html:text/html},
}

@misc{hoffman_no-u-turn_2011,
	title = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1111.4246},
	doi = {10.48550/arXiv.1111.4246},
	shorttitle = {The No-U-Turn Sampler},
	abstract = {Hamiltonian Monte Carlo ({HMC}) is a Markov chain Monte Carlo ({MCMC}) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many {MCMC} methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, {HMC}'s performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler ({NUTS}), an extension to {HMC} that eliminates the need to set a number of steps L. {NUTS} uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, {NUTS} perform at least as efficiently as and sometimes more efficiently than a well tuned standard {HMC} method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. {NUTS} can thus be used with no hand-tuning at all. {NUTS} is also suitable for applications such as {BUGS}-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
	number = {{arXiv}:1111.4246},
	publisher = {{arXiv}},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	urldate = {2023-04-02},
	date = {2011-11-17},
	eprinttype = {arxiv},
	eprint = {1111.4246 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/J8VHN5H2/Hoffman and Gelman - 2011 - The No-U-Turn Sampler Adaptively Setting Path Len.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/CJSJE9PL/1111.html:text/html},
}

@article{kingma_introduction_2019,
	title = {An Introduction to Variational Autoencoders},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	pages = {307--392},
	number = {4},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2023-04-02},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1906.02691 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/VT82DUCI/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/3AII2LUQ/1906.html:text/html},
}

@misc{xu_empirical_2015,
	title = {Empirical Evaluation of Rectified Activations in Convolutional Network},
	url = {http://arxiv.org/abs/1505.00853},
	doi = {10.48550/arXiv.1505.00853},
	abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit ({ReLU}), leaky rectified linear unit (Leaky {ReLU}), parametric rectified linear unit ({PReLU}) and a new randomized leaky rectified linear units ({RReLU}). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in {ReLU}. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using {RReLU}, we achieved 75.68{\textbackslash}\% accuracy on {CIFAR}-100 test set without multiple test or ensemble.},
	number = {{arXiv}:1505.00853},
	publisher = {{arXiv}},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	urldate = {2023-04-02},
	date = {2015-11-27},
	eprinttype = {arxiv},
	eprint = {1505.00853 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/WSVSK5GU/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/B8HS7DP4/1505.html:text/html},
}

@misc{durkan_neural_2019,
	title = {Neural Spline Flows},
	url = {http://arxiv.org/abs/1906.04032},
	doi = {10.48550/arXiv.1906.04032},
	abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
	number = {{arXiv}:1906.04032},
	publisher = {{arXiv}},
	author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
	urldate = {2023-04-02},
	date = {2019-12-02},
	eprinttype = {arxiv},
	eprint = {1906.04032 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/4A2MBXTY/Durkan et al. - 2019 - Neural Spline Flows.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/Z3HHQMA9/1906.html:text/html},
}

@misc{yuan_explainability_2021,
	title = {On Explainability of Graph Neural Networks via Subgraph Explorations},
	url = {http://arxiv.org/abs/2102.05152},
	doi = {10.48550/arXiv.2102.05152},
	abstract = {We consider the problem of explaining the predictions of graph neural networks ({GNNs}), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as {SubgraphX}, to explain {GNNs} by identifying important subgraphs. Given a trained {GNN} model and an input graph, our {SubgraphX} explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain {GNNs} via identifying subgraphs explicitly and directly. Experimental results show that our {SubgraphX} achieves significantly improved explanations, while keeping computations at a reasonable level.},
	number = {{arXiv}:2102.05152},
	publisher = {{arXiv}},
	author = {Yuan, Hao and Yu, Haiyang and Wang, Jie and Li, Kang and Ji, Shuiwang},
	urldate = {2023-04-02},
	date = {2021-05-31},
	eprinttype = {arxiv},
	eprint = {2102.05152 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/5UK8XQPY/Yuan et al. - 2021 - On Explainability of Graph Neural Networks via Sub.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/2NR9LFZU/2102.html:text/html},
}

@misc{lin_generative_2021,
	title = {Generative Causal Explanations for Graph Neural Networks},
	url = {http://arxiv.org/abs/2104.06643},
	doi = {10.48550/arXiv.2104.06643},
	abstract = {This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any {GNNs} on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of {GNNs} as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for {GNNs}, Gem explains {GNNs} on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the {GNNs} or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target {GNN} very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to \$30{\textbackslash}\%\$ and speeds up the explanation process by up to \$110{\textbackslash}times\$ as compared to its state-of-the-art alternatives.},
	number = {{arXiv}:2104.06643},
	publisher = {{arXiv}},
	author = {Lin, Wanyu and Lan, Hao and Li, Baochun},
	urldate = {2023-04-02},
	date = {2021-06-06},
	eprinttype = {arxiv},
	eprint = {2104.06643 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/XFXL86AU/Lin et al. - 2021 - Generative Causal Explanations for Graph Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/E478XWTP/2104.html:text/html},
}

@article{dibaeinia_sergio_2020,
	title = {{SERGIO}: A Single-Cell Expression Simulator Guided by Gene Regulatory Networks},
	volume = {11},
	issn = {2405-4712},
	url = {https://www.sciencedirect.com/science/article/pii/S2405471220302878},
	doi = {10.1016/j.cels.2020.08.003},
	shorttitle = {{SERGIO}},
	abstract = {A common approach to benchmarking of single-cell transcriptomics tools is to generate synthetic datasets that statistically resemble experimental data. However, most existing single-cell simulators do not incorporate transcription factor-gene regulatory interactions that underlie expression dynamics. Here, we present {SERGIO}, a simulator of single-cell gene expression data that models the stochastic nature of transcription as well as regulation of genes by multiple transcription factors according to a user-provided gene regulatory network. {SERGIO} can simulate any number of cell types in steady state or cells differentiating to multiple fates. We show that datasets generated by {SERGIO} are statistically comparable to experimental data generated by Illumina {HiSeq}2000, Drop-seq, Illumina 10X chromium, and Smart-seq. We use {SERGIO} to benchmark several single-cell analysis tools, including {GRN} inference methods, and identify Tcf7, Gata3, and Bcl11b as key drivers of T cell differentiation by performing in silico knockout experiments. {SERGIO} is freely available for download here: https://github.com/{PayamDiba}/{SERGIO}.},
	pages = {252--271.e11},
	number = {3},
	journaltitle = {Cell Systems},
	shortjournal = {Cell Systems},
	author = {Dibaeinia, Payam and Sinha, Saurabh},
	urldate = {2023-04-02},
	date = {2020-09-23},
	langid = {english},
	keywords = {benchmarking single-cell analysis tools, differentiation trajectories, gene regulatory networks, {RNA} velocity, simulations, single-cell {RNA}-seq},
	file = {ScienceDirect Full Text PDF:/Users/shalinpatel/Zotero/storage/W9WVCFG8/Dibaeinia and Sinha - 2020 - SERGIO A Single-Cell Expression Simulator Guided .pdf:application/pdf},
}

@online{weng_flow-based_2018,
	title = {Flow-based Deep Generative Models},
	url = {https://lilianweng.github.io/posts/2018-10-13-flow-models/},
	abstract = {So far, I’ve written about two types of generative models, {GAN} and {VAE}. Neither of them explicitly learns the probability density function of real data, \$p({\textbackslash}mathbf\{x\})\$ (where \${\textbackslash}mathbf\{x\} {\textbackslash}in {\textbackslash}mathcal\{D\}\$) — because it is really hard! Taking the generative model with latent variables as an example, \$p({\textbackslash}mathbf\{x\}) = {\textbackslash}int p({\textbackslash}mathbf\{x\}{\textbackslash}vert{\textbackslash}mathbf\{z\})p({\textbackslash}mathbf\{z\})d{\textbackslash}mathbf\{z\}\$ can hardly be calculated as it is intractable to go through all possible values of the latent code \${\textbackslash}mathbf\{z\}\$.
Flow-based deep generative models conquer this hard problem with the help of normalizing flows, a powerful statistics tool for density estimation.},
	author = {Weng, Lilian},
	urldate = {2023-04-03},
	date = {2018-10-13},
	langid = {english},
	note = {Section: posts},
	file = {Snapshot:/Users/shalinpatel/Zotero/storage/JW3BCXSW/2018-10-13-flow-models.html:text/html},
}

@article{gilbert_random_1959,
	title = {Random Graphs},
	volume = {30},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2237458},
	pages = {1141--1144},
	number = {4},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Gilbert, E. N.},
	urldate = {2023-04-04},
	date = {1959},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {JSTOR Full Text PDF:/Users/shalinpatel/Zotero/storage/3KRZ6XYK/Gilbert - 1959 - Random Graphs.pdf:application/pdf},
}

@misc{fey_fast_2019,
	title = {Fast Graph Representation Learning with {PyTorch} Geometric},
	url = {http://arxiv.org/abs/1903.02428},
	doi = {10.48550/arXiv.1903.02428},
	abstract = {We introduce {PyTorch} Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon {PyTorch}. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. {PyTorch} Geometric achieves high data throughput by leveraging sparse {GPU} acceleration, by providing dedicated {CUDA} kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	number = {{arXiv}:1903.02428},
	publisher = {{arXiv}},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	urldate = {2023-04-08},
	date = {2019-04-25},
	eprinttype = {arxiv},
	eprint = {1903.02428 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/3BWD47SH/Fey and Lenssen - 2019 - Fast Graph Representation Learning with PyTorch Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/9IPFFEZR/1903.html:text/html},
}

@online{noauthor_continuous_nodate,
	title = {Continuous Univariate Distributions, Volume 2, 2nd Edition {\textbar} Wiley},
	url = {https://www.wiley.com/en-us/Continuous+Univariate+Distributions%2C+Volume+2%2C+2nd+Edition-p-9780471584940},
	abstract = {Comprehensive reference for statistical distributions Continuous Univariate Distributions, Volume 2 provides in-depth reference for anyone who applies statistical distributions in fields including engineering, business, economics, and the sciences. Covering a range of distributions, both common and uncommon, this book includes guidance toward extreme value, logistics, Laplace, beta, rectangular, noncentral distributions and more. Each distribution is presented individually for ease of reference, with clear explanations of methods of inference, tolerance limits, applications, characterizations, and other important aspects, including reference to other related distributions.},
	titleaddon = {Wiley.com},
	urldate = {2023-04-08},
	langid = {english},
	file = {Snapshot:/Users/shalinpatel/Zotero/storage/H8R3W7H2/Continuous+Univariate+Distributions,+Volume+2,+2nd+Edition-p-9780471584940.html:text/html},
}

@misc{bolla_estimating_2017,
	title = {Estimating parameters of a directed weighted graph model with beta-distributed edge-weights},
	url = {http://arxiv.org/abs/1707.08904},
	doi = {10.48550/arXiv.1707.08904},
	abstract = {We introduce a directed, weighted random graph model, where the edge-weights are independent and beta-distributed with parameters depending on their endpoints. We will show that the row- and column-sums of the transformed edge-weight matrix are sufficient statistics for the parameters, and use the theory of exponential families to prove that the {ML} estimate of the parameters exists and is unique. Then an algorithm to find this estimate is introduced together with convergence proof that uses properties of the digamma function. Simulation results and applications are also presented.},
	number = {{arXiv}:1707.08904},
	publisher = {{arXiv}},
	author = {Bolla, Marianna and Elbanna, Ahmed and Mala, Jozsef},
	urldate = {2023-04-08},
	date = {2017-08-08},
	eprinttype = {arxiv},
	eprint = {1707.08904 [math, stat]},
	keywords = {62F10, 62B05, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/FTAFGJFN/Bolla et al. - 2017 - Estimating parameters of a directed weighted graph.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/V7Z3Q43N/1707.html:text/html},
}

@misc{dolatabadi_invertible_2020,
	title = {Invertible Generative Modeling using Linear Rational Splines},
	url = {http://arxiv.org/abs/2001.05168},
	doi = {10.48550/arXiv.2001.05168},
	abstract = {Normalizing flows attempt to model an arbitrary probability distribution through a set of invertible mappings. These transformations are required to achieve a tractable Jacobian determinant that can be used in high-dimensional scenarios. The first normalizing flow designs used coupling layer mappings built upon affine transformations. The significant advantage of such models is their easy-to-compute inverse. Nevertheless, making use of affine transformations may limit the expressiveness of such models. Recently, invertible piecewise polynomial functions as a replacement for affine transformations have attracted attention. However, these methods require solving a polynomial equation to calculate their inverse. In this paper, we explore using linear rational splines as a replacement for affine transformations used in coupling layers. Besides having a straightforward inverse, inference and generation have similar cost and architecture in this method. Moreover, simulation results demonstrate the competitiveness of this approach's performance compared to existing methods.},
	number = {{arXiv}:2001.05168},
	publisher = {{arXiv}},
	author = {Dolatabadi, Hadi M. and Erfani, Sarah and Leckie, Christopher},
	urldate = {2023-04-09},
	date = {2020-04-12},
	eprinttype = {arxiv},
	eprint = {2001.05168 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shalinpatel/Zotero/storage/GSASR2JW/Dolatabadi et al. - 2020 - Invertible Generative Modeling using Linear Ration.pdf:application/pdf;arXiv.org Snapshot:/Users/shalinpatel/Zotero/storage/8PURGTEM/2001.html:text/html},
}

@article{zhang_exploring_2005,
	title = {Exploring conditions for the optimality of naïve bayes},
	volume = {19},
	issn = {0218-0014},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001405003983},
	doi = {10.1142/S0218001405003983},
	abstract = {Naïve Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classification is surprising, because the conditional independence assumption on which it is based is rarely true in real-world applications. An open question is: what is the true reason for the surprisingly good performance of Naïve Bayes in classification?

In this paper, we propose a novel explanation for the good classification performance of Naïve Bayes. We show that, essentially, dependence distribution plays a crucial role. Here dependence distribution means how the local dependence of an attribute distributes in each class, evenly or unevenly, and how the local dependences of all attributes work together, consistently (supporting a certain classification) or inconsistently (canceling each other out). Specifically, we show that no matter how strong the dependences among attributes are, Naïve Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences cancel each other out. We propose and prove a sufficient and necessary condition for the optimality of Naïve Bayes. Further, we investigate the optimality of Naïve Bayes under the Gaussian distribution. We present and prove a sufficient condition for the optimality of Naïve Bayes, in which the dependences among attributes exist. This provides evidence that dependences may cancel each other out.

Our theoretic analysis can be used in designing learning algorithms. In fact, a major class of learning algorithms for Bayesian networks are conditional independence-based (or {CI}-based), which are essentially based on dependence. We design a dependence distribution-based algorithm by extending the {ChowLiu} algorithm, a widely used {CI} based algorithm. Our experiments show that the new algorithm outperforms the {ChowLiu} algorithm, which also provides empirical evidence to support our new explanation.},
	pages = {183--198},
	number = {2},
	journaltitle = {International Journal of Pattern Recognition and Artificial Intelligence},
	shortjournal = {Int. J. Patt. Recogn. Artif. Intell.},
	author = {Zhang, Harry},
	urldate = {2023-04-17},
	date = {2005-03},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {classification, Naïve Bayes, optimality},
	file = {Full Text PDF:/Users/shalinpatel/Zotero/storage/RJTLEAPQ/Zhang - 2005 - Exploring conditions for the optimality of naïve b.pdf:application/pdf},
}
